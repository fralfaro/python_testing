{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bienvenido a Python Testing El objetivo de este repositorio es dar una breve introducci\u00f3n sobre c\u00f3mo testear en Python (Test-driven development, Pytest, Coverage , etc.) Para esto tenemos la siguiente tabla de contenidos: Introducci\u00f3n al TDD Introducci\u00f3n Pytest Introducci\u00f3n Coverage Nota : El repositorio lo puede encontrar en el siguiente link: fralfaro/python_testing .","title":"Home"},{"location":"#bienvenido-a-python-testing","text":"El objetivo de este repositorio es dar una breve introducci\u00f3n sobre c\u00f3mo testear en Python (Test-driven development, Pytest, Coverage , etc.) Para esto tenemos la siguiente tabla de contenidos: Introducci\u00f3n al TDD Introducci\u00f3n Pytest Introducci\u00f3n Coverage Nota : El repositorio lo puede encontrar en el siguiente link: fralfaro/python_testing .","title":"Bienvenido a Python Testing"},{"location":"coverage/","text":"Coverage Introducci\u00f3n La cobertura (o coverage ) de un c\u00f3digo corresponde a una m\u00e9trica que determina la cantidad de l\u00edneas de c\u00f3digo que se validan con \u00e9xito en un procedimiento de testing, lo que a su vez ayuda a analizar en exhaustividad el software. Las empresas deben asegurarse de que el software que desarrollan cumpla con todas las caracter\u00edsticas esenciales de calidad: correcci\u00f3n confiabilidad eficacia, seguridad mantenibilidad. Beneficios El an\u00e1lisis de cobertura de c\u00f3digo solo se puede utilizar para la validaci\u00f3n de casos de tests que se ejecutan en el c\u00f3digo fuente y no para la evaluaci\u00f3n del producto de software. Adem\u00e1s, no eval\u00faa si el c\u00f3digo fuente est\u00e1 libre de errores ni prueba si un c\u00f3digo escrito es correcto. Algunos beneficios: F\u00e1cil mantenimiento del c\u00f3digo base : escribir c\u00f3digo escalable es fundamental para ampliar el programa de software mediante la introducci\u00f3n de funcionalidades nuevas o modificadas. Sin embargo, es dif\u00edcil determinar si el c\u00f3digo escrito es escalable. Exposici\u00f3n de c\u00f3digo incorrecto : el an\u00e1lisis continuo ayudar\u00e1 a los desarrolladores a comprender el c\u00f3digo incorrecto, muerto y sin usar. Tiempo de comercializaci\u00f3n m\u00e1s r\u00e1pido : con la ayuda de esta m\u00e9trica, los desarrolladores pueden terminar el proceso de desarrollo de software m\u00e1s r\u00e1pido, aumentando as\u00ed su productividad y eficiencia. \u00bfC\u00f3mo se mide? Para calcular el porcentaje de cobertura del c\u00f3digo, simplemente use la siguiente f\u00f3rmula: Porcentaje de cobertura de c\u00f3digo = (N\u00famero de l\u00edneas de c\u00f3digo ejecutadas por un algoritmo de prueba/N\u00famero total de l\u00edneas de c\u00f3digo en un componente del sistema) * 100. Pytest-Cov La librer\u00eda pytest-cov es el plugin utilizado para los informes de cobertura. En comparaci\u00f3n con el uso de coverage run , este complemento tiene algunos extras: Soporte de subprocesos : puede bifurcar o ejecutar cosas en un subproceso y estar\u00e1 cubierto sin ning\u00fan problema. Soporte de Xdist : puede usar todas las funciones de pytest-xdist y a\u00fan obtener cobertura. Comportamiento constante de Pytest . Si ejecuta la sentencia coverage run -m pytest , tendr\u00e1 sys.path ligeramente diferente (CWD estar\u00e1 en \u00e9l, a diferencia de cuando se ejecuta pytest ). Es posible generar cualquier combinaci\u00f3n de informes para una sola ejecuci\u00f3n de test. Los informes disponibles son terminales (con o sin los n\u00fameros de l\u00ednea que faltan), HTML, XML y c\u00f3digo fuente anotado. Casos de Usos Lo primero es replicar el ejemplo de la seci\u00f3n Pytest . Caso 01 El informe del terminal sin n\u00fameros de l\u00ednea (default): ! pytest -- cov - report term -- cov = src ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items src/tests/algo_test.py .... [ 57%] src/tests/srel_test.py ... [100%] ----------- coverage: platform linux, python 3.8.5-final-0 ----------- Name Stmts Miss Cover -------------------------------------------- src/tests/__init__.py 0 0 100% src/tests/algo_test.py 15 0 100% src/tests/srel_test.py 6 0 100% src/utils/__init__.py 0 0 100% src/utils/algo.py 24 0 100% src/utils/srel.py 2 0 100% -------------------------------------------- TOTAL 47 0 100% ============================== 7 passed in 0.09s =============================== Caso 02 El informe de la terminal con n\u00fameros de l\u00ednea: ! pytest -- cov - report term - missing -- cov = src ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items src/tests/algo_test.py .... [ 57%] src/tests/srel_test.py ... [100%] ----------- coverage: platform linux, python 3.8.5-final-0 ----------- Name Stmts Miss Cover Missing ------------------------------------------------------ src/tests/__init__.py 0 0 100% src/tests/algo_test.py 15 0 100% src/tests/srel_test.py 6 0 100% src/utils/__init__.py 0 0 100% src/utils/algo.py 24 0 100% src/utils/srel.py 2 0 100% ------------------------------------------------------ TOTAL 47 0 100% ============================== 7 passed in 0.10s =============================== Caso 03 El informe de la terminal con skip covered : ! pytest -- cov - report term : skip - covered -- cov = src ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items src/tests/algo_test.py .... [ 57%] src/tests/srel_test.py ... [100%] ----------- coverage: platform linux, python 3.8.5-final-0 ----------- Name Stmts Miss Cover --------------------------- --------------------------- TOTAL 47 0 100% 6 files skipped due to complete coverage. ============================== 7 passed in 0.10s =============================== Caso 04 Estas tres opciones de informe se env\u00edan a archivos sin mostrar nada en el terminal: ! pytest -- cov - report html \\ -- cov - report xml \\ -- cov - report annotate \\ -- cov = src ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items src/tests/algo_test.py .... [ 57%] src/tests/srel_test.py ... [100%] ----------- coverage: platform linux, python 3.8.5-final-0 ----------- Coverage annotated source written next to source Coverage HTML written to dir htmlcov Coverage XML written to file coverage.xml ============================== 7 passed in 0.15s =============================== Caso 05 Se puede especificar la ubicaci\u00f3n de salida para cada uno de estos informes. La ubicaci\u00f3n de salida del informe XML es un archivo. Donde, como ubicaci\u00f3n de salida para los informes HTML y de c\u00f3digo fuente anotado son directorios: ! pytest -- cov - report html : cov_html \\ -- cov - report xml : cov . xml \\ -- cov - report annotate : cov_annotate \\ -- cov = src ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items src/tests/algo_test.py .... [ 57%] src/tests/srel_test.py ... [100%] ----------- coverage: platform linux, python 3.8.5-final-0 ----------- Coverage annotated source written to dir cov_annotate Coverage HTML written to dir cov_html Coverage XML written to file cov.xml ============================== 7 passed in 0.17s =============================== Caso 06 La opci\u00f3n de informe final tambi\u00e9n puede suprimir la impresi\u00f3n en el terminal: ! pytest -- cov - report = -- cov = src ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items src/tests/algo_test.py .... [ 57%] src/tests/srel_test.py ... [100%] ============================== 7 passed in 0.12s =============================== Este modo puede ser especialmente \u00fatil en servidores de integraci\u00f3n continua, donde se necesita un archivo de cobertura para el procesamiento posterior, pero no es necesario ver un informe local. Por ejemplo, los tests realizados en Travis-CI podr\u00edan generar un archivo .coverage para usar con Coveralls . Porcentaje Ideal de cobertura Una sorprendente cobertura de c\u00f3digo del 100 % significa que el c\u00f3digo est\u00e1 100 % libre de errores. Ning\u00fan error indica que los casos de tests han cubierto todos los criterios y requisitos de la aplicaci\u00f3n de software. Entonces, si ese es el caso, \u00bfc\u00f3mo evaluamos si los scripts de tests han cumplido con una amplia gama de posibilidades? \u00bfQu\u00e9 pasa si los tests han cubierto los requisitos incorrectos? \u00bfQu\u00e9 pasa si los tests no cumplieron con algunos requisitos importantes? Entonces, \u00bfcu\u00e1l es el porcentaje de cobertura ideal que preguntas? El \u00fanico enfoque y objetivo de los desarrolladores y tester deber\u00eda ser escribir tests que no sean vagos. No concentrarse en lograr una cobertura del 100 por ciento. El an\u00e1lisis debe combinarse con tests robustos y escalables, que cubran todas las \u00e1reas funcionales y no funcionales del c\u00f3digo fuente.","title":"Coverage"},{"location":"coverage/#coverage","text":"","title":"Coverage"},{"location":"coverage/#introduccion","text":"La cobertura (o coverage ) de un c\u00f3digo corresponde a una m\u00e9trica que determina la cantidad de l\u00edneas de c\u00f3digo que se validan con \u00e9xito en un procedimiento de testing, lo que a su vez ayuda a analizar en exhaustividad el software. Las empresas deben asegurarse de que el software que desarrollan cumpla con todas las caracter\u00edsticas esenciales de calidad: correcci\u00f3n confiabilidad eficacia, seguridad mantenibilidad.","title":"Introducci\u00f3n"},{"location":"coverage/#beneficios","text":"El an\u00e1lisis de cobertura de c\u00f3digo solo se puede utilizar para la validaci\u00f3n de casos de tests que se ejecutan en el c\u00f3digo fuente y no para la evaluaci\u00f3n del producto de software. Adem\u00e1s, no eval\u00faa si el c\u00f3digo fuente est\u00e1 libre de errores ni prueba si un c\u00f3digo escrito es correcto. Algunos beneficios: F\u00e1cil mantenimiento del c\u00f3digo base : escribir c\u00f3digo escalable es fundamental para ampliar el programa de software mediante la introducci\u00f3n de funcionalidades nuevas o modificadas. Sin embargo, es dif\u00edcil determinar si el c\u00f3digo escrito es escalable. Exposici\u00f3n de c\u00f3digo incorrecto : el an\u00e1lisis continuo ayudar\u00e1 a los desarrolladores a comprender el c\u00f3digo incorrecto, muerto y sin usar. Tiempo de comercializaci\u00f3n m\u00e1s r\u00e1pido : con la ayuda de esta m\u00e9trica, los desarrolladores pueden terminar el proceso de desarrollo de software m\u00e1s r\u00e1pido, aumentando as\u00ed su productividad y eficiencia.","title":"Beneficios"},{"location":"coverage/#como-se-mide","text":"Para calcular el porcentaje de cobertura del c\u00f3digo, simplemente use la siguiente f\u00f3rmula: Porcentaje de cobertura de c\u00f3digo = (N\u00famero de l\u00edneas de c\u00f3digo ejecutadas por un algoritmo de prueba/N\u00famero total de l\u00edneas de c\u00f3digo en un componente del sistema) * 100.","title":"\u00bfC\u00f3mo se mide?"},{"location":"coverage/#pytest-cov","text":"La librer\u00eda pytest-cov es el plugin utilizado para los informes de cobertura. En comparaci\u00f3n con el uso de coverage run , este complemento tiene algunos extras: Soporte de subprocesos : puede bifurcar o ejecutar cosas en un subproceso y estar\u00e1 cubierto sin ning\u00fan problema. Soporte de Xdist : puede usar todas las funciones de pytest-xdist y a\u00fan obtener cobertura. Comportamiento constante de Pytest . Si ejecuta la sentencia coverage run -m pytest , tendr\u00e1 sys.path ligeramente diferente (CWD estar\u00e1 en \u00e9l, a diferencia de cuando se ejecuta pytest ). Es posible generar cualquier combinaci\u00f3n de informes para una sola ejecuci\u00f3n de test. Los informes disponibles son terminales (con o sin los n\u00fameros de l\u00ednea que faltan), HTML, XML y c\u00f3digo fuente anotado.","title":"Pytest-Cov"},{"location":"coverage/#casos-de-usos","text":"Lo primero es replicar el ejemplo de la seci\u00f3n Pytest .","title":"Casos de Usos"},{"location":"coverage/#caso-01","text":"El informe del terminal sin n\u00fameros de l\u00ednea (default): ! pytest -- cov - report term -- cov = src ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items src/tests/algo_test.py .... [ 57%] src/tests/srel_test.py ... [100%] ----------- coverage: platform linux, python 3.8.5-final-0 ----------- Name Stmts Miss Cover -------------------------------------------- src/tests/__init__.py 0 0 100% src/tests/algo_test.py 15 0 100% src/tests/srel_test.py 6 0 100% src/utils/__init__.py 0 0 100% src/utils/algo.py 24 0 100% src/utils/srel.py 2 0 100% -------------------------------------------- TOTAL 47 0 100% ============================== 7 passed in 0.09s ===============================","title":"Caso 01"},{"location":"coverage/#caso-02","text":"El informe de la terminal con n\u00fameros de l\u00ednea: ! pytest -- cov - report term - missing -- cov = src ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items src/tests/algo_test.py .... [ 57%] src/tests/srel_test.py ... [100%] ----------- coverage: platform linux, python 3.8.5-final-0 ----------- Name Stmts Miss Cover Missing ------------------------------------------------------ src/tests/__init__.py 0 0 100% src/tests/algo_test.py 15 0 100% src/tests/srel_test.py 6 0 100% src/utils/__init__.py 0 0 100% src/utils/algo.py 24 0 100% src/utils/srel.py 2 0 100% ------------------------------------------------------ TOTAL 47 0 100% ============================== 7 passed in 0.10s ===============================","title":"Caso 02"},{"location":"coverage/#caso-03","text":"El informe de la terminal con skip covered : ! pytest -- cov - report term : skip - covered -- cov = src ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items src/tests/algo_test.py .... [ 57%] src/tests/srel_test.py ... [100%] ----------- coverage: platform linux, python 3.8.5-final-0 ----------- Name Stmts Miss Cover --------------------------- --------------------------- TOTAL 47 0 100% 6 files skipped due to complete coverage. ============================== 7 passed in 0.10s ===============================","title":"Caso 03"},{"location":"coverage/#caso-04","text":"Estas tres opciones de informe se env\u00edan a archivos sin mostrar nada en el terminal: ! pytest -- cov - report html \\ -- cov - report xml \\ -- cov - report annotate \\ -- cov = src ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items src/tests/algo_test.py .... [ 57%] src/tests/srel_test.py ... [100%] ----------- coverage: platform linux, python 3.8.5-final-0 ----------- Coverage annotated source written next to source Coverage HTML written to dir htmlcov Coverage XML written to file coverage.xml ============================== 7 passed in 0.15s ===============================","title":"Caso 04"},{"location":"coverage/#caso-05","text":"Se puede especificar la ubicaci\u00f3n de salida para cada uno de estos informes. La ubicaci\u00f3n de salida del informe XML es un archivo. Donde, como ubicaci\u00f3n de salida para los informes HTML y de c\u00f3digo fuente anotado son directorios: ! pytest -- cov - report html : cov_html \\ -- cov - report xml : cov . xml \\ -- cov - report annotate : cov_annotate \\ -- cov = src ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items src/tests/algo_test.py .... [ 57%] src/tests/srel_test.py ... [100%] ----------- coverage: platform linux, python 3.8.5-final-0 ----------- Coverage annotated source written to dir cov_annotate Coverage HTML written to dir cov_html Coverage XML written to file cov.xml ============================== 7 passed in 0.17s ===============================","title":"Caso 05"},{"location":"coverage/#caso-06","text":"La opci\u00f3n de informe final tambi\u00e9n puede suprimir la impresi\u00f3n en el terminal: ! pytest -- cov - report = -- cov = src ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items src/tests/algo_test.py .... [ 57%] src/tests/srel_test.py ... [100%] ============================== 7 passed in 0.12s =============================== Este modo puede ser especialmente \u00fatil en servidores de integraci\u00f3n continua, donde se necesita un archivo de cobertura para el procesamiento posterior, pero no es necesario ver un informe local. Por ejemplo, los tests realizados en Travis-CI podr\u00edan generar un archivo .coverage para usar con Coveralls .","title":"Caso 06"},{"location":"coverage/#porcentaje-ideal-de-cobertura","text":"Una sorprendente cobertura de c\u00f3digo del 100 % significa que el c\u00f3digo est\u00e1 100 % libre de errores. Ning\u00fan error indica que los casos de tests han cubierto todos los criterios y requisitos de la aplicaci\u00f3n de software. Entonces, si ese es el caso, \u00bfc\u00f3mo evaluamos si los scripts de tests han cumplido con una amplia gama de posibilidades? \u00bfQu\u00e9 pasa si los tests han cubierto los requisitos incorrectos? \u00bfQu\u00e9 pasa si los tests no cumplieron con algunos requisitos importantes? Entonces, \u00bfcu\u00e1l es el porcentaje de cobertura ideal que preguntas? El \u00fanico enfoque y objetivo de los desarrolladores y tester deber\u00eda ser escribir tests que no sean vagos. No concentrarse en lograr una cobertura del 100 por ciento. El an\u00e1lisis debe combinarse con tests robustos y escalables, que cubran todas las \u00e1reas funcionales y no funcionales del c\u00f3digo fuente.","title":"Porcentaje Ideal de cobertura"},{"location":"intro/","text":"Test-driven development Introducci\u00f3n En palabras simples, el desarrollo guiado por pruebas pone las pruebas en el coraz\u00f3n de nuestro trabajo. En su forma m\u00e1s simple consiste en un proceso iterativo de 3 fases: Red : Escribir test que pruebe su funcionalidad y asegurar que falle. Green : Escribe el c\u00f3digo m\u00ednimo necesario para pasar el test, Refactor : Refactoriza de ser necesario, Ejemplo sencillo A modo de ejemplo, vamos a testear la funci\u00f3n paridad , que determina si un n\u00famero natural es par o no. Lo primero que se debe hacer es crear el test, para ello se ocupar\u00e1 la librer\u00eda pytest . Nota : No es necesario conocer previamente la librer\u00eda pytest para entender el ejemplo. @pytest . mark . parametrize ( \"number, expected\" , [ ( 2 , 'par' ), ]) def test_paridad ( number , expected ): assert paridad ( number ) == expected El test nos dice que si el input es el n\u00famero 2 , la funci\u00f3n paridad devuelve el output 'par' . C\u00f3mo a\u00fan no hemos escrito la funci\u00f3n, el test fallar\u00e1 ( fase red ). ========= test session starts ============================================ platform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/ds_blog plugins: anyio-3.3.0 collected 1 item temp/test_funcion.py F [100%] ========= 1 failed in 0.14s =============================================== Ahora, se escribe la funci\u00f3n paridad ( fase green ): def paridad ( n : int ) -> str : \"\"\" Determina si un numero natural es par o no. :param n: numero entero :return: 'par' si es el numero es par; 'impar' en otro caso \"\"\" return 'par' if n % 2 == 0 else 'impar' Volvemos a correr el test: ========= test session starts ============================================ platform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/ds_blog plugins: anyio-3.3.0 collected 1 item temp/test_funcion.py . [100%] ========= 1 passed in 0.06s =============================================== Hemos cometido un descuido a proposito, no hemos testeado el caso si el n\u00famero fuese impar, por lo cual reescribimos el test ( fase refactor ) @pytest . mark . parametrize ( \"number, expected\" , [ ( 2 , 'par' ), ( 3 , 'impar' ), ]) def test_paridad ( number , expected ): assert paridad ( number ) == expected y corremos nuevamente los test: ========= test session starts ============================================ platform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/ds_blog plugins: anyio-3.3.0 collected 2 items temp/test_funcion.py .. [100%] ========= 2 passed in 0.06s =============================================== Listo, nuestra funci\u00f3n paridad ha sido testeado correctamente!. \u00bfPorqu\u00e9 deber\u00eda usarlo? Existen varias razones por las que uno deber\u00eda usar TDD. Entre ellas podemos encontrar: Formular bien nuestros pensamientos mediante la escritura de un test significativo antes de ponernos a solucionar el problema nos ayuda a clarificar los l\u00edmites del problema y c\u00f3mo podemos resolverlo. Con el tiempo esto ayuda a obtener un dise\u00f1o modular y reusable del c\u00f3digo. Escribir tests ayuda la forma en que escribimos c\u00f3digo, haci\u00e9ndolo m\u00e1s legible a otros. Sin embargo, no es un acto de altruismo, la mayor\u00eda de las veces ese otro es tu futuro yo. Verifica que el c\u00f3digo funciona de la manera que se espera, y lo hace de forma autom\u00e1tica. Te permite realizar refactoring con la certeza de que no has roto nada. Los tests escritos sirven como documentaci\u00f3n para otros desarrolladores. Es una pr\u00e1ctica requerida en metodolog\u00edas de desarrollo de software agile . Evidencia emp\u00edrica El 2008, Nagappan, Maximilien, Bhat y Williams publicaron el paper llamado Realizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams , en donde estudiaron 4 equipos de trabajo (3 de Microsoft y 1 de IBM), con proyectos que variaban entre las 6000 lineas de c\u00f3digo hasta las 155k. Estas son parte de sus conclusiones: Todos los equipos demostraron una baja considerable en la densidad de defectos: 40% para el equipo de IBM, y entre 60-90% para los equipos de Microsoft. Como todo en la vida, nada es gratis: Incremento del tiempo de desarrollo var\u00eda entre un 15% a 35%. Sin embargo Desde un punto de vista de eficacia este incremento en tiempo de desarrollo se compensa por los costos de mantenci\u00f3n reducidos debido al incremento en calidad. Adem\u00e1s, es importante escribir tests junto con la implementaci\u00f3n en peque\u00f1as iteraciones. George y Williams encontraron que escribir tests despu\u00e9s de que la aplicaci\u00f3n est\u00e1 mas o menos lista hace que se testee menos porque los desarrolladores piensan en menos casos, y adem\u00e1s la aplicaci\u00f3n se vuelve menos testeable. Otra conclusi\u00f3n interesante del estudio de George y Williams es que un 79% de los desarrolladores experimentaron que el uso de TDD conlleva a un dise\u00f1o m\u00e1s simple. \u00bfPuedo usar TDD siempre? No, pero puedes usarlo casi siempre. El an\u00e1lisis exploratorio es un caso en que el uso de TDD no hace sentido. Una vez que tenemos definido el problema a solucionar y un mejor entendimiento del problema podemos aterrizar nuestras ideas a la implementaci\u00f3n v\u00eda testing. Librer\u00edas disponibles Ac\u00e1 listamos algunas librer\u00edas de TDD en Python: unittest : M\u00f3dulo dentro de la librer\u00eda est\u00e1ndar de Python. Permite realizar tests unitarios, de integraci\u00f3n y end to end. doctest : Permite realizar test de la documentaci\u00f3n del c\u00f3digo (ejemplos: Numpy o Pandas ). pytest : Librer\u00eda de testing ampliamente usada en proyectos nuevos de Python. nose : Librer\u00eda que extiende unittest para hacerlo m\u00e1s simple. coverage : Herramienta para medir la cobertura de c\u00f3digo de los proyectos. tox : Herramienta para facilitar el test de una librer\u00eda en diferentes versiones e int\u00e9rpretes de Python. hypothesis : Librer\u00eda para escribir tests v\u00eda reglas que ayuda a encontrar casos borde. behave : Permite utilizar Behavior Driven Development , un proceso de desarrollo derivado del TDD. Knowledge base / Lecturas recomendadas Realizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams , es una buena lectura, sobretodo los consejos que dan en las conclusiones. Google Testing Blog : Poseen varios art\u00edculos sobre c\u00f3mo abordar problemas tipo, buenas pr\u00e1cticas de dise\u00f1o para generar c\u00f3digo testeable, entre otros. En particular destaca la serie Testing on the Toilet . Cualquier art\u00edculo de Martin Fowler sobre testing , empezando por \u00e9ste Design Patterns : Los patrones de dise\u00f1o de software tienen en consideraci\u00f3n que el c\u00f3digo sea testeable .","title":"TDD"},{"location":"intro/#test-driven-development","text":"","title":"Test-driven development"},{"location":"intro/#introduccion","text":"En palabras simples, el desarrollo guiado por pruebas pone las pruebas en el coraz\u00f3n de nuestro trabajo. En su forma m\u00e1s simple consiste en un proceso iterativo de 3 fases: Red : Escribir test que pruebe su funcionalidad y asegurar que falle. Green : Escribe el c\u00f3digo m\u00ednimo necesario para pasar el test, Refactor : Refactoriza de ser necesario,","title":"Introducci\u00f3n"},{"location":"intro/#ejemplo-sencillo","text":"A modo de ejemplo, vamos a testear la funci\u00f3n paridad , que determina si un n\u00famero natural es par o no. Lo primero que se debe hacer es crear el test, para ello se ocupar\u00e1 la librer\u00eda pytest . Nota : No es necesario conocer previamente la librer\u00eda pytest para entender el ejemplo. @pytest . mark . parametrize ( \"number, expected\" , [ ( 2 , 'par' ), ]) def test_paridad ( number , expected ): assert paridad ( number ) == expected El test nos dice que si el input es el n\u00famero 2 , la funci\u00f3n paridad devuelve el output 'par' . C\u00f3mo a\u00fan no hemos escrito la funci\u00f3n, el test fallar\u00e1 ( fase red ). ========= test session starts ============================================ platform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/ds_blog plugins: anyio-3.3.0 collected 1 item temp/test_funcion.py F [100%] ========= 1 failed in 0.14s =============================================== Ahora, se escribe la funci\u00f3n paridad ( fase green ): def paridad ( n : int ) -> str : \"\"\" Determina si un numero natural es par o no. :param n: numero entero :return: 'par' si es el numero es par; 'impar' en otro caso \"\"\" return 'par' if n % 2 == 0 else 'impar' Volvemos a correr el test: ========= test session starts ============================================ platform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/ds_blog plugins: anyio-3.3.0 collected 1 item temp/test_funcion.py . [100%] ========= 1 passed in 0.06s =============================================== Hemos cometido un descuido a proposito, no hemos testeado el caso si el n\u00famero fuese impar, por lo cual reescribimos el test ( fase refactor ) @pytest . mark . parametrize ( \"number, expected\" , [ ( 2 , 'par' ), ( 3 , 'impar' ), ]) def test_paridad ( number , expected ): assert paridad ( number ) == expected y corremos nuevamente los test: ========= test session starts ============================================ platform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/ds_blog plugins: anyio-3.3.0 collected 2 items temp/test_funcion.py .. [100%] ========= 2 passed in 0.06s =============================================== Listo, nuestra funci\u00f3n paridad ha sido testeado correctamente!.","title":"Ejemplo sencillo"},{"location":"intro/#porque-deberia-usarlo","text":"Existen varias razones por las que uno deber\u00eda usar TDD. Entre ellas podemos encontrar: Formular bien nuestros pensamientos mediante la escritura de un test significativo antes de ponernos a solucionar el problema nos ayuda a clarificar los l\u00edmites del problema y c\u00f3mo podemos resolverlo. Con el tiempo esto ayuda a obtener un dise\u00f1o modular y reusable del c\u00f3digo. Escribir tests ayuda la forma en que escribimos c\u00f3digo, haci\u00e9ndolo m\u00e1s legible a otros. Sin embargo, no es un acto de altruismo, la mayor\u00eda de las veces ese otro es tu futuro yo. Verifica que el c\u00f3digo funciona de la manera que se espera, y lo hace de forma autom\u00e1tica. Te permite realizar refactoring con la certeza de que no has roto nada. Los tests escritos sirven como documentaci\u00f3n para otros desarrolladores. Es una pr\u00e1ctica requerida en metodolog\u00edas de desarrollo de software agile .","title":"\u00bfPorqu\u00e9 deber\u00eda usarlo?"},{"location":"intro/#evidencia-empirica","text":"El 2008, Nagappan, Maximilien, Bhat y Williams publicaron el paper llamado Realizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams , en donde estudiaron 4 equipos de trabajo (3 de Microsoft y 1 de IBM), con proyectos que variaban entre las 6000 lineas de c\u00f3digo hasta las 155k. Estas son parte de sus conclusiones: Todos los equipos demostraron una baja considerable en la densidad de defectos: 40% para el equipo de IBM, y entre 60-90% para los equipos de Microsoft. Como todo en la vida, nada es gratis: Incremento del tiempo de desarrollo var\u00eda entre un 15% a 35%. Sin embargo Desde un punto de vista de eficacia este incremento en tiempo de desarrollo se compensa por los costos de mantenci\u00f3n reducidos debido al incremento en calidad. Adem\u00e1s, es importante escribir tests junto con la implementaci\u00f3n en peque\u00f1as iteraciones. George y Williams encontraron que escribir tests despu\u00e9s de que la aplicaci\u00f3n est\u00e1 mas o menos lista hace que se testee menos porque los desarrolladores piensan en menos casos, y adem\u00e1s la aplicaci\u00f3n se vuelve menos testeable. Otra conclusi\u00f3n interesante del estudio de George y Williams es que un 79% de los desarrolladores experimentaron que el uso de TDD conlleva a un dise\u00f1o m\u00e1s simple.","title":"Evidencia emp\u00edrica"},{"location":"intro/#puedo-usar-tdd-siempre","text":"No, pero puedes usarlo casi siempre. El an\u00e1lisis exploratorio es un caso en que el uso de TDD no hace sentido. Una vez que tenemos definido el problema a solucionar y un mejor entendimiento del problema podemos aterrizar nuestras ideas a la implementaci\u00f3n v\u00eda testing.","title":"\u00bfPuedo usar TDD siempre?"},{"location":"intro/#librerias-disponibles","text":"Ac\u00e1 listamos algunas librer\u00edas de TDD en Python: unittest : M\u00f3dulo dentro de la librer\u00eda est\u00e1ndar de Python. Permite realizar tests unitarios, de integraci\u00f3n y end to end. doctest : Permite realizar test de la documentaci\u00f3n del c\u00f3digo (ejemplos: Numpy o Pandas ). pytest : Librer\u00eda de testing ampliamente usada en proyectos nuevos de Python. nose : Librer\u00eda que extiende unittest para hacerlo m\u00e1s simple. coverage : Herramienta para medir la cobertura de c\u00f3digo de los proyectos. tox : Herramienta para facilitar el test de una librer\u00eda en diferentes versiones e int\u00e9rpretes de Python. hypothesis : Librer\u00eda para escribir tests v\u00eda reglas que ayuda a encontrar casos borde. behave : Permite utilizar Behavior Driven Development , un proceso de desarrollo derivado del TDD.","title":"Librer\u00edas disponibles"},{"location":"intro/#knowledge-base-lecturas-recomendadas","text":"Realizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams , es una buena lectura, sobretodo los consejos que dan en las conclusiones. Google Testing Blog : Poseen varios art\u00edculos sobre c\u00f3mo abordar problemas tipo, buenas pr\u00e1cticas de dise\u00f1o para generar c\u00f3digo testeable, entre otros. En particular destaca la serie Testing on the Toilet . Cualquier art\u00edculo de Martin Fowler sobre testing , empezando por \u00e9ste Design Patterns : Los patrones de dise\u00f1o de software tienen en consideraci\u00f3n que el c\u00f3digo sea testeable .","title":"Knowledge base / Lecturas recomendadas"},{"location":"pytest/","text":"Pytest Introducci\u00f3n El framework pytest facilita la escritura de peque\u00f1os tests, pero escala para admitir test funcionales complejas para aplicaciones y librerias. Si ha escrito pruebas unitarias para su c\u00f3digo Python antes, es posible que haya utilizado el m\u00f3dulo unittest integrado de Python. unittest proporciona una base s\u00f3lida sobre la cual construir su suite de pruebas, pero tiene algunas deficiencias. Varios marcos de prueba de terceros intentan abordar algunos de los problemas con unittest, y pytest ha demostrado ser uno de los m\u00e1s populares. pytest es un ecosistema basado en complementos y rico en funciones para probar su c\u00f3digo Python. Uso de Pytest Si no se especifican argumentos, los archivos de tests se buscan en ubicaciones desde las rutas de tests (si est\u00e1n configuradas) o el directorio actual. Alternativamente, los argumentos de la l\u00ednea de comandos se pueden utilizar en cualquier combinaci\u00f3n de directorios, nombres de archivos o ID de nodo. En los directorios seleccionados, pytest busca archivos de test _ *.py o *_test.py . En los archivos seleccionados, pytest busca funciones de test con prefijo test. Veamos un ejemplo sencillo de esto: a) Escribir funciones a testear %% writefile algo . py def max ( values ): _max = values [ 0 ] for val in values : if val > _max : _max = val return _max def min ( values ): _min = values [ 0 ] for val in values : if val < _min : _min = val return _min b) Escribir los tests %% writefile test_min_max . py import algo def test_min (): values = ( 2 , 3 , 1 , 4 , 6 ) val = algo . min ( values ) assert val == 1 def test_max (): values = ( 2 , 3 , 1 , 4 , 6 ) val = algo . max ( values ) assert val == 6 El archivo test_min_max.py tiene una palabra de test en su nombre. Nos sirve para diferenciar entre un script de python gen\u00e9rico respecto a uno de testeo. ! pytest test_min_max . py ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collecting ... collected 2 items test_min_max.py .. [100%] ============================== 2 passed in 0.02s =============================== Esta es la salida. Hubo dos pruebas y ambas han pasado con \u00e9xito. Se muestra una salida m\u00e1s detallada con pytest -v test_min_max.py . Pytest skip Con el decorador de skip , podemos omitir los test especificados. Hay varias razones para saltarse el test; por ejemplo, una base de datos/servicio en l\u00ednea no est\u00e1 disponible en este momento o nos saltamos los test espec\u00edficas de Linux en Windows. %% writefile skipping . py import algo import pytest @pytest . mark . skip def test_min (): values = ( 2 , 3 , 1 , 4 , 6 ) val = algo . min ( values ) assert val == 1 def test_max (): values = ( 2 , 3 , 1 , 4 , 6 ) val = algo . max ( values ) assert val == 6 En el ejemplo, se omite test_min() . ! pytest skipping . py ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 2 items skipping.py s. [100%] ========================= 1 passed, 1 skipped in 0.02s ========================= Pytest Xfail Podemos usar el marcador xfail para indicar que espera que falle una prueba. Un caso de uso com\u00fan para esto es cuando encuentra un error en su software y escribe una prueba para documentar c\u00f3mo deber\u00eda comportarse el software. Esta prueba, por supuesto, fallar\u00e1 hasta que corrija el error. Para evitar tener una prueba fallida, marca la prueba como xfail . Una vez que se corrige el error, elimina el marcador xfail y tiene una prueba de regresi\u00f3n que asegura que el error no se repita. %% writefile xfail . py import pytest xfail = pytest . mark . xfail @xfail def test_hello (): assert 0 @xfail ( run = False ) def test_hello2 (): assert 0 @xfail ( \"hasattr(os, 'sep')\" ) def test_hello3 (): assert 0 @xfail ( reason = \"bug 110\" ) def test_hello4 (): assert 0 @xfail ( 'pytest.__version__[0] != \"17\"' ) def test_hello5 (): assert 0 def test_hello6 (): pytest . xfail ( \"reason\" ) @xfail ( raises = IndexError ) def test_hello7 (): x = [] x [ 1 ] = 1 En este caso, todos los test ser\u00e1n ignorados (con una x ), lo cu\u00e1l no afectar\u00e1 al resto de los tests (suponiendo que los tests pasen correctamente). ! pytest xfail . py ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items xfail.py xxxxxxx [100%] ============================== 7 xfailed in 0.13s ============================== Pytest marking Podemos usar marcadores para organizar los test unitarios. %% writefile marking . py import pytest @pytest . mark . a def test_a1 (): assert ( 1 ) == ( 1 ) @pytest . mark . a def test_a2 (): assert ( 1 , 2 ) == ( 1 , 2 ) @pytest . mark . a def test_a3 (): assert ( 1 , 2 , 3 ) == ( 1 , 2 , 3 ) @pytest . mark . b def test_b1 (): assert \"falcon\" == \"fal\" + \"con\" @pytest . mark . b def test_b2 (): assert \"falcon\" == f \"fal { 'con' } \" Tenemos dos grupos de tests identificados por marcadores, \\(a\\) y \\(b\\) . Estas unidades son ejecutadas por pytest -m a marking.py y pytest -m b marking.py . ! pytest - m a marking . py ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collecting ... collected 5 items / 2 deselected / 3 selected marking.py ... [100%] =============================== warnings summary =============================== marking.py:3 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:3: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.a marking.py:8 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:8: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.a marking.py:13 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.a marking.py:18 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.b - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.b marking.py:23 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:23: PytestUnknownMarkWarning: Unknown pytest.mark.b - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.b -- Docs: https://docs.pytest.org/en/latest/warnings.html ================= 3 passed, 2 deselected, 5 warnings in 0.02s ================== ! pytest - m b marking . py ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collecting ... collected 5 items / 3 deselected / 2 selected marking.py .. [100%] =============================== warnings summary =============================== marking.py:3 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:3: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.a marking.py:8 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:8: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.a marking.py:13 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.a marking.py:18 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.b - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.b marking.py:23 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:23: PytestUnknownMarkWarning: Unknown pytest.mark.b - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.b -- Docs: https://docs.pytest.org/en/latest/warnings.html ================= 2 passed, 3 deselected, 5 warnings in 0.02s ================== Pytest parameterized tests Con los tests parametrizados, podemos agregar m\u00faltiples valores a nuestras afirmaciones. Usamos el decorador @pytest.mark.parametrize . %% writefile parameterized . py import algo import pytest @pytest . mark . parametrize ( \"data, expected\" , [(( 2 , 3 , 1 , 4 , 6 ), 1 ), (( 5 , - 2 , 0 , 9 , 12 ), - 2 ), (( 200 , 100 , 0 , 300 , 400 ), 0 )]) def test_min ( data , expected ): val = algo . min ( data ) assert val == expected @pytest . mark . parametrize ( \"data, expected\" , [(( 2 , 3 , 1 , 4 , 6 ), 6 ), (( 5 , - 2 , 0 , 9 , 12 ), 12 ), (( 200 , 100 , 0 , 300 , 400 ), 400 )]) def test_max ( data , expected ): val = algo . max ( data ) assert val == expected Pasamos dos valores a la funci\u00f3n de testeo: los datos y el valor esperado. En nuestro caso, probamos la funci\u00f3n min() con tres tuplas de datos. ! pytest parameterized . py ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 6 items parameterized.py ...... [100%] ============================== 6 passed in 0.03s =============================== Pytest fixtures Los tests deben ejecutarse en el contexto de un conjunto conocido de objetos. Este conjunto de objetos se denomina test fixture . %% writefile algo2 . py def sel_sort ( data ): if not isinstance ( data , list ): vals = list ( data ) else : vals = data size = len ( vals ) for i in range ( 0 , size ): for j in range ( i + 1 , size ): if vals [ j ] < vals [ i ]: _min = vals [ j ] vals [ j ] = vals [ i ] vals [ i ] = _min return vals %% writefile fixtures . py import algo2 import pytest # fijar valor data @pytest . fixture def data (): return [ 3 , 2 , 1 , 5 , - 3 , 2 , 0 , - 2 , 11 , 9 ] # pasar data como argumento del test def test_sel_sort ( data ): sorted_vals = algo2 . sel_sort ( data ) assert sorted_vals == sorted ( data ) ! pytest fixtures . py ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collecting ... collected 1 item fixtures.py . [100%] ============================== 1 passed in 0.02s =============================== Observaci\u00f3n : para realizar los siguientes ejemplos, es necesario eliminar todos los archivos .py previos. # eliminar archivos .py ! rm *. py Pytest layouts Los tests de Python se pueden organizar de varias formas. Los tests se pueden integrar en el paquete de Python o pueden descansar fuera de la librer\u00eda. Integrated tests A continuaci\u00f3n, mostramos c\u00f3mo ejecutar pruebas dentro de un paquete de Python. setup.py utils \u2502 algo.py \u2502 srel.py \u2502 __init__.py \u2502 \u2514\u2500\u2500\u2500tests algo_test.py srel_test.py __init__.py Tenemos este dise\u00f1o del paquete. Los test se encuentran en el subdirectorio de tests dentro del paquete. a) crear archivos previos del ejemplos # crear carpetas ! mkdir utils tests # agregar archivo __init__.py ! touch utils / __init__ . py # agregar archivo __init__.py ! touch tests / __init__ . py b) crear archivos .py a testear %% writefile setup . py from setuptools import setup , find_packages setup ( name = \"utils\" , packages = find_packages ()) %% writefile utils / algo . py def sel_sort ( data ): if not isinstance ( data , list ): vals = list ( data ) else : vals = data size = len ( vals ) for i in range ( 0 , size ): for j in range ( i + 1 , size ): if vals [ j ] < vals [ i ]: _min = vals [ j ] vals [ j ] = vals [ i ] vals [ i ] = _min return vals def max ( values ): _max = values [ 0 ] for val in values : if val > _max : _max = val return _max def min ( values ): _min = values [ 0 ] for val in values : if val < _min : _min = val return _min %% writefile utils / srel . py def is_palindrome ( val ): return val == val [:: - 1 ] c) crear los tests %% writefile tests / algo_test . py import utils.algo import pytest @pytest . mark . parametrize ( \"data\" , [ [ 3 , 2 , 1 , 5 , - 3 , 2 , 0 , - 2 , 11 , 9 ], ( 3 , 2 , 1 , 5 , - 3 , 2 , 0 , - 2 , 11 , 9 ) ] ) def test_sel_sort ( data ): sorted_vals = utils . algo . sel_sort ( data ) assert sorted_vals == sorted ( data ) @pytest . fixture def values (): return ( 2 , 3 , 1 , 4 , 6 ) def test_min ( values ): val = utils . algo . min ( values ) assert val == 1 def test_max ( values ): val = utils . algo . max ( values ) assert val == 6 %% writefile tests / srel_test . py import utils.srel import pytest @pytest . mark . parametrize ( \"word, expected\" , [ ( 'kayak' , True ), ( 'civic' , True ), ( 'forest' , False ) ] ) def test_palindrome ( word , expected ): val = utils . srel . is_palindrome ( word ) assert val == expected # dejar tests en la ruta correcta ! mv tests utils d) correr los test ! ! pytest -- pyargs utils ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items utils/tests/algo_test.py .... [ 57%] utils/tests/srel_test.py ... [100%] ============================== 7 passed in 0.04s =============================== Tests outside the package El siguiente ejemplo muestra un dise\u00f1o dela aplicaci\u00f3n donde los tests no est\u00e1n integrados dentro del paquete. setup.py src \u2514\u2500\u2500\u2500utils \u2502 algo.py \u2502 srel.py tests algo_test.py srel_test.py En este dise\u00f1o, tenemos tets fuera del \u00e1rbol de fuentes. a) crear dise\u00f1o del ejemplo # crear carpeta src ! mkdir src # mover carpeta utils ! mv utils src # mover carpeta tests ! mv src / utils / tests src / b) Correr los test ! ! pytest ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collecting ... collected 7 items src/tests/algo_test.py .... [ 57%] src/tests/srel_test.py ... [100%] ============================== 7 passed in 0.04s ===============================","title":"Pytest"},{"location":"pytest/#pytest","text":"","title":"Pytest"},{"location":"pytest/#introduccion","text":"El framework pytest facilita la escritura de peque\u00f1os tests, pero escala para admitir test funcionales complejas para aplicaciones y librerias. Si ha escrito pruebas unitarias para su c\u00f3digo Python antes, es posible que haya utilizado el m\u00f3dulo unittest integrado de Python. unittest proporciona una base s\u00f3lida sobre la cual construir su suite de pruebas, pero tiene algunas deficiencias. Varios marcos de prueba de terceros intentan abordar algunos de los problemas con unittest, y pytest ha demostrado ser uno de los m\u00e1s populares. pytest es un ecosistema basado en complementos y rico en funciones para probar su c\u00f3digo Python.","title":"Introducci\u00f3n"},{"location":"pytest/#uso-de-pytest","text":"Si no se especifican argumentos, los archivos de tests se buscan en ubicaciones desde las rutas de tests (si est\u00e1n configuradas) o el directorio actual. Alternativamente, los argumentos de la l\u00ednea de comandos se pueden utilizar en cualquier combinaci\u00f3n de directorios, nombres de archivos o ID de nodo. En los directorios seleccionados, pytest busca archivos de test _ *.py o *_test.py . En los archivos seleccionados, pytest busca funciones de test con prefijo test. Veamos un ejemplo sencillo de esto: a) Escribir funciones a testear %% writefile algo . py def max ( values ): _max = values [ 0 ] for val in values : if val > _max : _max = val return _max def min ( values ): _min = values [ 0 ] for val in values : if val < _min : _min = val return _min b) Escribir los tests %% writefile test_min_max . py import algo def test_min (): values = ( 2 , 3 , 1 , 4 , 6 ) val = algo . min ( values ) assert val == 1 def test_max (): values = ( 2 , 3 , 1 , 4 , 6 ) val = algo . max ( values ) assert val == 6 El archivo test_min_max.py tiene una palabra de test en su nombre. Nos sirve para diferenciar entre un script de python gen\u00e9rico respecto a uno de testeo. ! pytest test_min_max . py ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collecting ... collected 2 items test_min_max.py .. [100%] ============================== 2 passed in 0.02s =============================== Esta es la salida. Hubo dos pruebas y ambas han pasado con \u00e9xito. Se muestra una salida m\u00e1s detallada con pytest -v test_min_max.py .","title":"Uso de Pytest"},{"location":"pytest/#pytest-skip","text":"Con el decorador de skip , podemos omitir los test especificados. Hay varias razones para saltarse el test; por ejemplo, una base de datos/servicio en l\u00ednea no est\u00e1 disponible en este momento o nos saltamos los test espec\u00edficas de Linux en Windows. %% writefile skipping . py import algo import pytest @pytest . mark . skip def test_min (): values = ( 2 , 3 , 1 , 4 , 6 ) val = algo . min ( values ) assert val == 1 def test_max (): values = ( 2 , 3 , 1 , 4 , 6 ) val = algo . max ( values ) assert val == 6 En el ejemplo, se omite test_min() . ! pytest skipping . py ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 2 items skipping.py s. [100%] ========================= 1 passed, 1 skipped in 0.02s =========================","title":"Pytest skip"},{"location":"pytest/#pytest-xfail","text":"Podemos usar el marcador xfail para indicar que espera que falle una prueba. Un caso de uso com\u00fan para esto es cuando encuentra un error en su software y escribe una prueba para documentar c\u00f3mo deber\u00eda comportarse el software. Esta prueba, por supuesto, fallar\u00e1 hasta que corrija el error. Para evitar tener una prueba fallida, marca la prueba como xfail . Una vez que se corrige el error, elimina el marcador xfail y tiene una prueba de regresi\u00f3n que asegura que el error no se repita. %% writefile xfail . py import pytest xfail = pytest . mark . xfail @xfail def test_hello (): assert 0 @xfail ( run = False ) def test_hello2 (): assert 0 @xfail ( \"hasattr(os, 'sep')\" ) def test_hello3 (): assert 0 @xfail ( reason = \"bug 110\" ) def test_hello4 (): assert 0 @xfail ( 'pytest.__version__[0] != \"17\"' ) def test_hello5 (): assert 0 def test_hello6 (): pytest . xfail ( \"reason\" ) @xfail ( raises = IndexError ) def test_hello7 (): x = [] x [ 1 ] = 1 En este caso, todos los test ser\u00e1n ignorados (con una x ), lo cu\u00e1l no afectar\u00e1 al resto de los tests (suponiendo que los tests pasen correctamente). ! pytest xfail . py ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items xfail.py xxxxxxx [100%] ============================== 7 xfailed in 0.13s ==============================","title":"Pytest Xfail"},{"location":"pytest/#pytest-marking","text":"Podemos usar marcadores para organizar los test unitarios. %% writefile marking . py import pytest @pytest . mark . a def test_a1 (): assert ( 1 ) == ( 1 ) @pytest . mark . a def test_a2 (): assert ( 1 , 2 ) == ( 1 , 2 ) @pytest . mark . a def test_a3 (): assert ( 1 , 2 , 3 ) == ( 1 , 2 , 3 ) @pytest . mark . b def test_b1 (): assert \"falcon\" == \"fal\" + \"con\" @pytest . mark . b def test_b2 (): assert \"falcon\" == f \"fal { 'con' } \" Tenemos dos grupos de tests identificados por marcadores, \\(a\\) y \\(b\\) . Estas unidades son ejecutadas por pytest -m a marking.py y pytest -m b marking.py . ! pytest - m a marking . py ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collecting ... collected 5 items / 2 deselected / 3 selected marking.py ... [100%] =============================== warnings summary =============================== marking.py:3 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:3: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.a marking.py:8 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:8: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.a marking.py:13 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.a marking.py:18 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.b - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.b marking.py:23 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:23: PytestUnknownMarkWarning: Unknown pytest.mark.b - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.b -- Docs: https://docs.pytest.org/en/latest/warnings.html ================= 3 passed, 2 deselected, 5 warnings in 0.02s ================== ! pytest - m b marking . py ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collecting ... collected 5 items / 3 deselected / 2 selected marking.py .. [100%] =============================== warnings summary =============================== marking.py:3 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:3: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.a marking.py:8 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:8: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.a marking.py:13 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.a marking.py:18 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.b - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.b marking.py:23 /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:23: PytestUnknownMarkWarning: Unknown pytest.mark.b - is this a typo? You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html @pytest.mark.b -- Docs: https://docs.pytest.org/en/latest/warnings.html ================= 2 passed, 3 deselected, 5 warnings in 0.02s ==================","title":"Pytest marking"},{"location":"pytest/#pytest-parameterized-tests","text":"Con los tests parametrizados, podemos agregar m\u00faltiples valores a nuestras afirmaciones. Usamos el decorador @pytest.mark.parametrize . %% writefile parameterized . py import algo import pytest @pytest . mark . parametrize ( \"data, expected\" , [(( 2 , 3 , 1 , 4 , 6 ), 1 ), (( 5 , - 2 , 0 , 9 , 12 ), - 2 ), (( 200 , 100 , 0 , 300 , 400 ), 0 )]) def test_min ( data , expected ): val = algo . min ( data ) assert val == expected @pytest . mark . parametrize ( \"data, expected\" , [(( 2 , 3 , 1 , 4 , 6 ), 6 ), (( 5 , - 2 , 0 , 9 , 12 ), 12 ), (( 200 , 100 , 0 , 300 , 400 ), 400 )]) def test_max ( data , expected ): val = algo . max ( data ) assert val == expected Pasamos dos valores a la funci\u00f3n de testeo: los datos y el valor esperado. En nuestro caso, probamos la funci\u00f3n min() con tres tuplas de datos. ! pytest parameterized . py ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 6 items parameterized.py ...... [100%] ============================== 6 passed in 0.03s ===============================","title":"Pytest parameterized tests"},{"location":"pytest/#pytest-fixtures","text":"Los tests deben ejecutarse en el contexto de un conjunto conocido de objetos. Este conjunto de objetos se denomina test fixture . %% writefile algo2 . py def sel_sort ( data ): if not isinstance ( data , list ): vals = list ( data ) else : vals = data size = len ( vals ) for i in range ( 0 , size ): for j in range ( i + 1 , size ): if vals [ j ] < vals [ i ]: _min = vals [ j ] vals [ j ] = vals [ i ] vals [ i ] = _min return vals %% writefile fixtures . py import algo2 import pytest # fijar valor data @pytest . fixture def data (): return [ 3 , 2 , 1 , 5 , - 3 , 2 , 0 , - 2 , 11 , 9 ] # pasar data como argumento del test def test_sel_sort ( data ): sorted_vals = algo2 . sel_sort ( data ) assert sorted_vals == sorted ( data ) ! pytest fixtures . py ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collecting ... collected 1 item fixtures.py . [100%] ============================== 1 passed in 0.02s =============================== Observaci\u00f3n : para realizar los siguientes ejemplos, es necesario eliminar todos los archivos .py previos. # eliminar archivos .py ! rm *. py","title":"Pytest fixtures"},{"location":"pytest/#pytest-layouts","text":"Los tests de Python se pueden organizar de varias formas. Los tests se pueden integrar en el paquete de Python o pueden descansar fuera de la librer\u00eda.","title":"Pytest layouts"},{"location":"pytest/#integrated-tests","text":"A continuaci\u00f3n, mostramos c\u00f3mo ejecutar pruebas dentro de un paquete de Python. setup.py utils \u2502 algo.py \u2502 srel.py \u2502 __init__.py \u2502 \u2514\u2500\u2500\u2500tests algo_test.py srel_test.py __init__.py Tenemos este dise\u00f1o del paquete. Los test se encuentran en el subdirectorio de tests dentro del paquete.","title":"Integrated tests"},{"location":"pytest/#a-crear-archivos-previos-del-ejemplos","text":"# crear carpetas ! mkdir utils tests # agregar archivo __init__.py ! touch utils / __init__ . py # agregar archivo __init__.py ! touch tests / __init__ . py","title":"a) crear archivos previos del ejemplos"},{"location":"pytest/#b-crear-archivos-py-a-testear","text":"%% writefile setup . py from setuptools import setup , find_packages setup ( name = \"utils\" , packages = find_packages ()) %% writefile utils / algo . py def sel_sort ( data ): if not isinstance ( data , list ): vals = list ( data ) else : vals = data size = len ( vals ) for i in range ( 0 , size ): for j in range ( i + 1 , size ): if vals [ j ] < vals [ i ]: _min = vals [ j ] vals [ j ] = vals [ i ] vals [ i ] = _min return vals def max ( values ): _max = values [ 0 ] for val in values : if val > _max : _max = val return _max def min ( values ): _min = values [ 0 ] for val in values : if val < _min : _min = val return _min %% writefile utils / srel . py def is_palindrome ( val ): return val == val [:: - 1 ]","title":"b) crear archivos .py a testear"},{"location":"pytest/#c-crear-los-tests","text":"%% writefile tests / algo_test . py import utils.algo import pytest @pytest . mark . parametrize ( \"data\" , [ [ 3 , 2 , 1 , 5 , - 3 , 2 , 0 , - 2 , 11 , 9 ], ( 3 , 2 , 1 , 5 , - 3 , 2 , 0 , - 2 , 11 , 9 ) ] ) def test_sel_sort ( data ): sorted_vals = utils . algo . sel_sort ( data ) assert sorted_vals == sorted ( data ) @pytest . fixture def values (): return ( 2 , 3 , 1 , 4 , 6 ) def test_min ( values ): val = utils . algo . min ( values ) assert val == 1 def test_max ( values ): val = utils . algo . max ( values ) assert val == 6 %% writefile tests / srel_test . py import utils.srel import pytest @pytest . mark . parametrize ( \"word, expected\" , [ ( 'kayak' , True ), ( 'civic' , True ), ( 'forest' , False ) ] ) def test_palindrome ( word , expected ): val = utils . srel . is_palindrome ( word ) assert val == expected # dejar tests en la ruta correcta ! mv tests utils","title":"c) crear los tests"},{"location":"pytest/#d-correr-los-test","text":"! pytest -- pyargs utils ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collected 7 items utils/tests/algo_test.py .... [ 57%] utils/tests/srel_test.py ... [100%] ============================== 7 passed in 0.04s ===============================","title":"d) correr los test !"},{"location":"pytest/#tests-outside-the-package","text":"El siguiente ejemplo muestra un dise\u00f1o dela aplicaci\u00f3n donde los tests no est\u00e1n integrados dentro del paquete. setup.py src \u2514\u2500\u2500\u2500utils \u2502 algo.py \u2502 srel.py tests algo_test.py srel_test.py En este dise\u00f1o, tenemos tets fuera del \u00e1rbol de fuentes.","title":"Tests outside the package"},{"location":"pytest/#a-crear-diseno-del-ejemplo","text":"# crear carpeta src ! mkdir src # mover carpeta utils ! mv utils src # mover carpeta tests ! mv src / utils / tests src /","title":"a) crear dise\u00f1o del ejemplo"},{"location":"pytest/#b-correr-los-test","text":"! pytest ============================= test session starts ============================== platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing plugins: hypothesis-5.49.0, cov-2.11.1 collecting ... collected 7 items src/tests/algo_test.py .... [ 57%] src/tests/srel_test.py ... [100%] ============================== 7 passed in 0.04s ===============================","title":"b) Correr los test !"}]}